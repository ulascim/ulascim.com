<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="icon" type="image/png" href="../favicon.png">
    <link rel="apple-touch-icon" href="../apple-touch-icon.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Fine-Tuning (SFT) | Research Report</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        /* ============================================
           BASE - Same design system as tokenizer-research
           ============================================ */
        * { margin: 0; padding: 0; box-sizing: border-box; }
        :root {
            --bg: #f3f3f3;
            --fg: #000000;
            --gray-100: #f3f4f6;
            --gray-300: #d1d5db;
            --positive: #16a34a;
            --negative: #dc2626;
            --accent: #2563eb;
            --turquoise: #00b5ad;
        }
        body {
            font-family: 'IBM Plex Mono', monospace;
            background: var(--bg);
            color: var(--fg);
            font-size: 12px;
            line-height: 1.3;
        }

        /* ============================================
           NAVIGATION
           ============================================ */
        .nav {
            position: sticky;
            top: 0;
            z-index: 50;
            border-bottom: 2px solid var(--fg);
            background: var(--bg);
        }
        .nav-container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 64px;
            height: 56px;
            display: flex;
            align-items: center;
        }
        .nav-left { display: flex; align-items: center; gap: 12px; }
        .nav-logo {
            font-size: 18px;
            font-weight: 700;
            letter-spacing: 2px;
            text-decoration: none;
            color: var(--fg);
        }
        .nav-logo .u-char { color: var(--turquoise); }
        .nav-center {
            position: absolute;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            align-items: center;
            gap: 24px;
        }
        .nav-link {
            color: var(--fg);
            text-decoration: none;
            font-size: 14px;
            font-weight: 700;
            letter-spacing: 0.05em;
        }
        .nav-link:hover, .nav-link.active { color: var(--accent); }
        .nav-divider { color: var(--fg); font-size: 14px; font-weight: 300; }
        .nav-right { margin-left: auto; }
        .nav-link-small { color: var(--fg); text-decoration: underline; font-size: 11px; }

        /* ============================================
           REPORT
           ============================================ */
        .report { max-width: 1400px; margin: 0 auto; padding: 24px 64px; }
        .report h1 { font-size: 28px; margin-bottom: 8px; letter-spacing: 2px; }
        .report h2 { font-size: 18px; margin-top: 36px; margin-bottom: 12px; border-bottom: 2px solid #000; padding-bottom: 4px; scroll-margin-top: 72px; }
        .report h3 { font-size: 14px; margin-top: 20px; margin-bottom: 8px; }
        .report p { font-size: 13px; line-height: 1.6; margin-bottom: 12px; }
        .report ul { font-size: 13px; margin: 12px 0; padding-left: 20px; }
        .report li { margin-bottom: 6px; line-height: 1.5; }
        .report .subtitle { font-size: 14px; color: #666; margin-bottom: 4px; }
        .report .authors { font-size: 12px; color: #888; margin-bottom: 32px; }
        .report code { background: #e5e5e0; padding: 1px 5px; font-size: 12px; }

        .report .abstract { background: #f5f5f0; padding: 16px; margin: 20px 0; border-left: 3px solid #000; }
        .report .finding { background: #fffbe6; padding: 12px; margin: 12px 0; border: 1px solid #e6d600; }
        .report .warning { background: #fee2e2; padding: 12px; margin: 12px 0; border: 1px solid #dc2626; }
        .report .discovery { background: #e6ffe6; padding: 12px; margin: 12px 0; border: 1px solid #0a0; }
        .report .insight { background: #eff6ff; padding: 12px; margin: 12px 0; border: 1px solid #2563eb; }

        .stat-grid { display: grid; grid-template-columns: repeat(4, 1fr); gap: 12px; margin: 16px 0; }
        .stat-box { background: #f3f4f6; padding: 12px; text-align: center; border: 2px solid #000; }
        .stat-box .value { font-size: 24px; font-weight: bold; }
        .stat-box .label { font-size: 10px; color: #666; margin-top: 2px; }
        .stat-grid-3 { display: grid; grid-template-columns: repeat(3, 1fr); gap: 12px; margin: 16px 0; }
        .stat-grid-5 { display: grid; grid-template-columns: repeat(5, 1fr); gap: 12px; margin: 16px 0; }
        .stat-grid-6 { display: grid; grid-template-columns: repeat(6, 1fr); gap: 12px; margin: 16px 0; }

        .report table { width: auto; border-collapse: collapse; border: 2px solid #000; margin: 16px 0; }
        .report tr:first-child { background: #f3f4f6; border-bottom: 2px solid #000; }
        .report th { padding: 4px 10px; text-align: left; font-size: 0.65rem; font-weight: 700; text-transform: uppercase; letter-spacing: 0.03em; white-space: nowrap; border-right: 2px solid #000; background: #f3f4f6; }
        .report th:last-child { border-right: none; }
        .report tr:not(:first-child) { border-bottom: 1px solid #d1d5db; }
        .report tr:last-child { border-bottom: none; }
        .report td { padding: 4px 10px; font-size: 0.75rem; white-space: nowrap; border-right: 2px solid #000; }
        .report td:last-child { border-right: none; }
        .report .highlight { background: #f5f5f0; }
        .report tr:not(:first-child):hover { background: rgba(0, 0, 0, 0.02); }

        .good { color: #16a34a; font-weight: 700; }
        .bad { color: #dc2626; font-weight: 700; }
        .neutral { color: #666; }

        .status { display: inline-block; padding: 2px 6px; font-size: 9px; font-weight: 700; }
        .status.complete { background: #dcfce7; color: #166534; }
        .status.progress { background: #fef9c3; color: #854d0e; }
        .status.next { background: #dbeafe; color: #1e40af; }

        .back-link { display: inline-block; font-size: 12px; color: #666; text-decoration: none; margin-bottom: 24px; padding: 8px 0; }
        .back-link:hover { color: #000; }
        .back-link::before { content: "\2190  "; }

        .report pre {
            background: #1a1a2e; color: #e0e0e0; padding: 16px; margin: 12px 0;
            font-size: 12px; line-height: 1.5; overflow-x: auto; border: 2px solid #000;
        }
        .report pre .comment { color: #6a9955; }
        .report pre .keyword { color: #569cd6; }
        .report pre .string { color: #ce9178; }
        .report pre .special { color: #d7ba7d; }

        .flow { display: flex; align-items: center; gap: 8px; flex-wrap: wrap; margin: 16px 0; }
        .flow-box { border: 2px solid #000; padding: 8px 14px; font-size: 12px; font-weight: 600; }
        .flow-box.done { background: #dcfce7; }
        .flow-box.active { background: #fef9c3; }
        .flow-box.next { background: #dbeafe; }
        .flow-box.pending { background: #f3f4f6; }
        .flow-arrow { font-size: 18px; font-weight: bold; }

        .sample-box { background: #fff; border: 2px solid #000; padding: 16px; margin: 12px 0; }
        .sample-box .sample-q { font-weight: 700; margin-bottom: 8px; font-size: 13px; }
        .sample-box .sample-a { font-size: 13px; line-height: 1.6; color: #333; }
        .sample-box .sample-label { font-size: 10px; color: #888; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 8px; }

        @media (max-width: 768px) {
            .nav-center { display: none; }
            .report { padding: 16px; }
            .stat-grid, .stat-grid-3, .stat-grid-5, .stat-grid-6 { grid-template-columns: repeat(2, 1fr); }
            .nav-container { padding: 0 16px; }
            .flow { flex-direction: column; }
        }
    </style>
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <div class="nav-left">
                <a href="token-sequencer.html" class="nav-logo">RESEARCH</a>
            </div>
            <div class="nav-right">
                <a href="sft-training.html" class="nav-link-small" style="font-weight: 700;">EN</a>
            </div>
        </div>
    </nav>

    <main class="report">
        <a href="token-sequencer.html" class="back-link">Back to Research</a>
        <h1>SUPERVISED FINE-TUNING (SFT)</h1>
        <p class="subtitle">Phase 4 &mdash; From 3,790 to 10,000 QA Pairs: Two Generations of ERP Assistant Training</p>
        <p class="authors">February 2026 &bull; Independent Research &bull; <span class="status progress">IN PROGRESS</span></p>

        <div class="stat-grid">
            <div class="stat-box">
                <div class="value good">3,790</div>
                <div class="label">SFT TRAINING EXAMPLES</div>
            </div>
            <div class="stat-box">
                <div class="value good">39.5%</div>
                <div class="label">VAL LOSS IMPROVEMENT</div>
            </div>
            <div class="stat-box">
                <div class="value">100s</div>
                <div class="label">TOTAL TRAINING TIME</div>
            </div>
            <div class="stat-box">
                <div class="value">639</div>
                <div class="label">TRAINING STEPS</div>
            </div>
        </div>

        <div class="abstract">
            <strong>Abstract.</strong> This report documents two generations of supervised fine-tuning (SFT) for
            Turkish ERP assistants built from scratch. <strong>V1</strong> (24.7M params, 512-token context) used
            Claude Opus 4.5 to generate 3,790 QA pairs from 320 ERP documentation chunks, proving that SFT works:
            validation loss dropped from 5.12 to 3.10 in 100 seconds on an H100 GPU. <strong>V2</strong> (67.6M params,
            2048-token context) redesigns the entire pipeline: 11 grouping strategies produce 707 chunk configurations
            from 532 source chunks; Claude Sonnet 4.6 (selected via 4-way API comparison) generates ~8,000&ndash;10,000
            diverse QA pairs with factual, procedural, comparative, inferential, and list-type questions at three
            difficulty levels. Every training example includes the source context chunk, training the model as a
            RAG context converter rather than a knowledge memorizer. Sections 1&ndash;12 document the completed v1
            experiment; Section 13 documents the v2 redesign currently in progress.
        </div>

        <div style="background: #fff; border: 2px solid #000; padding: 20px 28px; margin: 24px 0;">
            <h3 style="margin: 0 0 12px 0; font-size: 14px; letter-spacing: 1px;">TABLE OF CONTENTS</h3>
            <div style="columns: 2; column-gap: 32px; font-size: 12px; line-height: 2;">
                <a href="#sec-1" style="text-decoration: none; color: var(--fg); display: block;"><strong>1.</strong> The Full Pipeline</a>
                <a href="#sec-2" style="text-decoration: none; color: var(--fg); display: block;"><strong>2.</strong> Pretraining History</a>
                <a href="#sec-3" style="text-decoration: none; color: var(--fg); display: block;"><strong>3.</strong> ERP Documentation Source</a>
                <a href="#sec-4" style="text-decoration: none; color: var(--fg); display: block;"><strong>4.</strong> Synthetic Data Generation (V1)</a>
                <a href="#sec-5" style="text-decoration: none; color: var(--fg); display: block;"><strong>5.</strong> Prompt Engineering (V1)</a>
                <a href="#sec-6" style="text-decoration: none; color: var(--fg); display: block;"><strong>6.</strong> V1 QA Pair Statistics</a>
                <a href="#sec-7" style="text-decoration: none; color: var(--fg); display: block;"><strong>7.</strong> Chat Template &amp; Tokenization</a>
                <a href="#sec-8" style="text-decoration: none; color: var(--fg); display: block;"><strong>8.</strong> Loss Masking Strategy</a>
                <a href="#sec-9" style="text-decoration: none; color: var(--fg); display: block;"><strong>9.</strong> V1 SFT Training Configuration</a>
                <a href="#sec-10" style="text-decoration: none; color: var(--fg); display: block;"><strong>10.</strong> V1 Results</a>
                <a href="#sec-11" style="text-decoration: none; color: var(--fg); display: block;"><strong>11.</strong> V1 Model Outputs</a>
                <a href="#sec-12" style="text-decoration: none; color: var(--fg); display: block;"><strong>12.</strong> Analysis &amp; Lessons Learned</a>
                <a href="#sec-13" style="text-decoration: none; color: var(--fg); display: block;"><strong>13.</strong> V2 SFT Pipeline Redesign</a>
                <a href="#sec-14" style="text-decoration: none; color: var(--fg); display: block;"><strong>14.</strong> Reproducibility</a>
            </div>
        </div>

        <!-- ============================================ -->
        <h2 id="sec-1">1. THE FULL PIPELINE</h2>
        <!-- ============================================ -->

        <p>The model follows a standard modern LLM training pipeline. Each phase builds on the previous one, 
        progressively narrowing the model&rsquo;s capabilities from general language understanding to domain-specific 
        instruction following.</p>

        <p>Two generations of SFT are documented here: v1 (24.7M, proof of concept) and v2 (67.6M, production pipeline).</p>

        <div class="flow">
            <div class="flow-box done">TOKENIZER (64K BPE)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">V1: 24.7M (R1+R2)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">V1 SFT (3,790 pairs)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">V2: 67.6M (2048 ctx)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box active">V2 SFT (~10K pairs)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box next">DEPLOY</div>
        </div>

        <table>
            <tr><th>Phase</th><th>Purpose</th><th>Data</th><th>Result</th></tr>
            <tr><td>Tokenizer</td><td>Efficient Turkish text encoding</td><td>22 GB, 11 domains</td><td>64K vocab, 2.7&times; vs GPT-4</td></tr>
            <tr><td>V1 Architecture</td><td>Initial model design</td><td>&mdash;</td><td>24.7M params, ALiBi/GQA/SwiGLU</td></tr>
            <tr><td>Pretrain R1+R2</td><td>Basic &rarr; deep Turkish</td><td>22 GB corpus</td><td>278K steps, loss 3.46</td></tr>
            <tr><td>V1 SFT</td><td>ERP domain proof-of-concept</td><td>3,790 QA pairs (Opus 4.5)</td><td>639 steps, val loss 5.12 &rarr; 3.10</td></tr>
            <tr><td>Pretrain R2.5</td><td>2048-token context for RAG</td><td>22 GB corpus</td><td>228K steps, loss 3.22 (best)</td></tr>
            <tr><td>V2 Architecture</td><td>RAG-optimized model</td><td>&mdash;</td><td>67.6M params, d_model=512, 2:1 GQA</td></tr>
            <tr><td>V2 Pretrain</td><td>Full 67.6M pretraining</td><td>22 GB corpus</td><td><span class="status progress">IN PROGRESS</span></td></tr>
            <tr class="highlight"><td><strong>V2 SFT data</strong></td><td><strong>RAG-grounded ERP assistant</strong></td><td><strong>7,595 pairs (Sonnet 4.6)</strong></td><td><strong><span class="status complete">COMPLETE</span></strong></td></tr>
            <tr><td>RL (optional)</td><td>DPO / RLVR if SFT insufficient</td><td>TBD</td><td><span class="status next">FUTURE</span></td></tr>
        </table>

        <!-- ============================================ -->
        <h2 id="sec-2">2. PRETRAINING HISTORY</h2>
        <!-- ============================================ -->

        <p>The v1 model went through three pretraining rounds before SFT. Round 2 (shown below) established
        deep language understanding. Round 2.5 later extended context to 2048 tokens for RAG.
        The v2 model (67.6M) is a separate architecture pretrained from scratch.
        Full pretraining details are in the <a href="architecture-pretraining.html">Architecture &amp; Pretraining</a> report.</p>

        <div class="stat-grid-5">
            <div class="stat-box">
                <div class="value">228K</div>
                <div class="label">TOTAL STEPS</div>
            </div>
            <div class="stat-box">
                <div class="value">14.9B</div>
                <div class="label">TOKENS PROCESSED</div>
            </div>
            <div class="stat-box">
                <div class="value">10.5h</div>
                <div class="label">TRAINING TIME</div>
            </div>
            <div class="stat-box">
                <div class="value good">3.46</div>
                <div class="label">FINAL LOSS</div>
            </div>
            <div class="stat-box">
                <div class="value good">3.39</div>
                <div class="label">BEST LOSS</div>
            </div>
        </div>

        <h3>Loss curve progression</h3>
        <table>
            <tr><th>Step</th><th>Loss</th><th>LR</th><th>Tok/s</th><th>Sample Quality</th></tr>
            <tr><td>50,000 (R1)</td><td>2.62</td><td>3.0e-04</td><td>~16K (MPS)</td><td>Basic Turkish grammar</td></tr>
            <tr><td>95,000</td><td>~3.60</td><td>2.0e-04</td><td>~399K (H100)</td><td>Simple sentences, some repetition</td></tr>
            <tr><td>145,000</td><td>~3.50</td><td>1.1e-04</td><td>~401K</td><td>Coherent sentences with context</td></tr>
            <tr><td>195,000</td><td>~3.47</td><td>4.4e-05</td><td>~402K</td><td>Factual content: &ldquo;Kocaeli&rsquo;ndeyiz&rdquo;</td></tr>
            <tr class="highlight"><td><strong>228,000</strong></td><td class="good"><strong>3.46</strong></td><td>3.0e-05</td><td>~403K</td><td><strong>Real-world knowledge, correct grammar</strong></td></tr>
        </table>

        <div class="finding">
            <strong>Note on loss difference:</strong> Round 1 loss (2.62) and Round 2 loss (3.46) are not directly comparable.
            Round 1 trained on a ~500MB curated subset; Round 2 trained on 22GB of diverse text spanning 11 domains. 
            The higher absolute loss reflects the much harder prediction task across legal, medical, financial, 
            news, and literary text &mdash; not a regression in model quality. Sample outputs confirm dramatically 
            improved language understanding.
        </div>

        <h3>Sample evolution during Round 2</h3>
        <div class="sample-box">
            <div class="sample-label">Step 95,000</div>
            <div class="sample-a" style="color: #999;">&ldquo;Merhaba deyin! Bu e-postay&#305; seviyorum! Bu e-postan&#305;n amac&#305;, bu e-postan&#305;n ana no&hellip;&rdquo;</div>
        </div>
        <div class="sample-box">
            <div class="sample-label">Step 145,000</div>
            <div class="sample-a">&ldquo;T&uuml;rkiye T&uuml;rkiye&rsquo;de ve d&uuml;nyada ekonomik geli&#351;meler a&ccedil;&#305;s&#305;ndan b&uuml;y&uuml;k &ouml;nem ta&#351;&#305;maktad&#305;r&rdquo;</div>
        </div>
        <div class="sample-box">
            <div class="sample-label">Step 195,000</div>
            <div class="sample-a" style="color: #16a34a; font-weight: 600;">&ldquo;T&uuml;rkiye&rsquo;nin en b&uuml;y&uuml;k ikinci sanayi &#351;ehri konumundaki Kocaeli&rsquo;ndeyiz. En &ouml;nemli t&hellip;&rdquo;</div>
        </div>
        <div class="sample-box">
            <div class="sample-label">Step 200,000 &mdash; Best loss 3.39</div>
            <div class="sample-a" style="color: #16a34a; font-weight: 600;">&ldquo;T&uuml;rkiye&rsquo;de t&uuml;m d&uuml;nyada &lsquo;insan haklar&#305;&rsquo;ndan s&ouml;z edildi&#287;i gibi, T&uuml;rkiye&rsquo;de de, ulu&hellip;&rdquo;</div>
        </div>

        <div class="insight">
            <strong>Scaling law observation:</strong> At 26M parameters, the model is capacity-limited, not data-limited. 
            Chinchilla-optimal training for 26M params is ~520M tokens (20&times; params), but the model processed 14.9B tokens 
            (~573&times; params). Loss was still decreasing at step 228K but with diminishing returns &mdash; the model 
            had exhausted most of its representational capacity.
        </div>

        <div class="finding">
            <strong>What happened next:</strong> Round 2.5 retrained with <code>max_seq_len=2048</code> and
            <code>dropout=0.02</code> for RAG compatibility, achieving best loss 3.22 in 26.5 hours.
            The v2 architecture (67.6M, <code>d_model=512</code>, 2:1 GQA) was then designed to break the
            capacity ceiling &mdash; 4.2&times; more transformer parameters. See
            <a href="architecture-pretraining.html#sec-15">Section 15</a> and
            <a href="architecture-pretraining.html#sec-16">Section 16</a> of the Architecture report.
        </div>

        <!-- ============================================ -->
        <h2 id="sec-3">3. ERP DOCUMENTATION SOURCE</h2>
        <!-- ============================================ -->

        <p>The SFT training data was generated from the Solen Kablo ERP system documentation &mdash; the same system 
        the model is being built to assist with. The documentation was already pre-processed into structured chunks 
        with rich metadata as part of a RAG (Retrieval-Augmented Generation) pipeline.</p>

        <div class="stat-grid">
            <div class="stat-box">
                <div class="value">1,074</div>
                <div class="label">TOTAL CHUNKS</div>
            </div>
            <div class="stat-box">
                <div class="value">8</div>
                <div class="label">ERP MODULES</div>
            </div>
            <div class="stat-box">
                <div class="value">215K</div>
                <div class="label">TOTAL TOKENS</div>
            </div>
            <div class="stat-box">
                <div class="value">202</div>
                <div class="label">AVG TOKENS/CHUNK</div>
            </div>
        </div>

        <h3>Modules covered</h3>
        <table>
            <tr><th>Module</th><th>Description</th><th>Chunks (TR)</th><th>Chunks (EN)</th></tr>
            <tr><td><strong>Admin</strong></td><td>User management, roles, authentication, system settings</td><td>~80</td><td>~80</td></tr>
            <tr><td><strong>Hammadde</strong></td><td>Raw materials: purchase orders, QR tracking, supplier management</td><td>~90</td><td>~85</td></tr>
            <tr><td><strong>Stok</strong></td><td>Inventory: warehouse management, stock levels, movements</td><td>~85</td><td>~85</td></tr>
            <tr><td><strong>Teknik</strong></td><td>Cable database: specifications, standards, production recipes</td><td>~95</td><td>~90</td></tr>
            <tr><td><strong>Lab</strong></td><td>Quality control: test procedures, measurements, certificates</td><td>~60</td><td>~55</td></tr>
            <tr><td><strong>&Uuml;retim</strong></td><td>Production: work orders, machine management, scheduling</td><td>~50</td><td>~50</td></tr>
            <tr><td><strong>Sat&#305;&#351;</strong></td><td>Sales: customer orders, quotations, delivery tracking</td><td>~40</td><td>~45</td></tr>
            <tr><td><strong>Finans</strong></td><td>Finance: invoicing, payments, cost analysis</td><td>~32</td><td>~52</td></tr>
            <tr class="highlight"><td><strong>Total</strong></td><td></td><td><strong>532</strong></td><td><strong>542</strong></td></tr>
        </table>

        <h3>Chunk metadata</h3>
        <p>Each chunk contains structured metadata used to guide the QA generation:</p>
        <table>
            <tr><th>Field</th><th>Purpose</th><th>Example</th></tr>
            <tr><td><code>chunk_id</code></td><td>Unique identifier for resume capability</td><td><code>erp-mod-hammadde-tr_chunk_042</code></td></tr>
            <tr><td><code>module</code></td><td>ERP module name</td><td><code>Hammadde (Raw Materials Management)</code></td></tr>
            <tr><td><code>section_heading</code></td><td>Documentation section</td><td><code>Sipari&#351; Y&ouml;netimi</code></td></tr>
            <tr><td><code>breadcrumb</code></td><td>Navigation path</td><td><code>Hammadde &gt; Sipari&#351;ler &gt; Yeni Sipari&#351;</code></td></tr>
            <tr><td><code>language</code></td><td>Source language</td><td><code>tr</code> or <code>en</code></td></tr>
            <tr><td><code>token_count</code></td><td>Chunk size (for filtering)</td><td><code>186</code></td></tr>
            <tr><td><code>has_table</code></td><td>Contains tabular data</td><td><code>true</code></td></tr>
            <tr><td><code>has_code</code></td><td>Contains code/API references</td><td><code>false</code></td></tr>
            <tr><td><code>references_modules</code></td><td>Cross-module references</td><td><code>["Stok", "&Uuml;retim"]</code></td></tr>
        </table>

        <!-- ============================================ -->
        <h2 id="sec-4">4. SYNTHETIC DATA GENERATION (V1)</h2>
        <!-- ============================================ -->

        <p>The central challenge of SFT for a domain-specific model is data. Hand-writing thousands of QA pairs is 
        impractical; using generic instruction datasets would not teach the model about Solen Kablo&rsquo;s ERP system.
        The solution: use a large cloud LLM as a <strong>data conversion tool</strong> &mdash; not a knowledge source &mdash; 
        to transform existing ERP documentation into training-ready QA pairs.</p>

        <div class="finding">
            <strong>V1 vs V2 pipeline.</strong> This section describes the v1 approach (Claude Opus 4.5, individual chunks,
            3,790 pairs). The v2 pipeline (Claude Sonnet 4.6, 11 grouping rules, ~10K pairs) is documented in
            <a href="#sec-13">Section 13</a>.
        </div>

        <div class="flow">
            <div class="flow-box done">ERP Docs (HTML)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">Chunk + Metadata</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">Claude Opus 4.5</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">QA Pairs (JSONL)</div>
            <span class="flow-arrow">&rarr;</span>
            <div class="flow-box done">ChatML Format</div>
        </div>

        <h3>V1 design decisions</h3>
        <table>
            <tr><th>Decision</th><th>V1 Choice</th><th>V2 Change</th></tr>
            <tr><td><strong>LLM</strong></td><td>Claude Opus 4.5</td><td class="good">Claude Sonnet 4.6 (better question diversity)</td></tr>
            <tr><td><strong>Grouping</strong></td><td>Individual chunks only</td><td class="good">11 rules: individual, submodule, module, cross-module, etc.</td></tr>
            <tr><td><strong>Target model</strong></td><td>24.7M, 512 context</td><td class="good">67.6M, 2048 context</td></tr>
            <tr><td><strong>Focus</strong></td><td>User-centric questions</td><td>Same + technical questions (no audience restriction)</td></tr>
            <tr><td><strong>Format</strong></td><td>Single + multi-turn (70/30)</td><td>Single-turn only (RAG: one question, one answer)</td></tr>
            <tr><td><strong>Difficulty</strong></td><td>Graded, length-calibrated for 26M</td><td>Graded, max 200 words (uncapped format)</td></tr>
            <tr><td><strong>Volume</strong></td><td>3,790 pairs from 320 chunks</td><td class="good">~8&ndash;10K pairs from 707 groups</td></tr>
        </table>

        <div class="warning">
            <strong>Anti-hallucination rule.</strong> The LLM was explicitly instructed: &ldquo;HER cevap DO&#286;RUDAN verilen 
            metin par&ccedil;as&#305;ndan gelmeli. Metinde OLMAYAN bilgi EKLEME &mdash; hi&ccedil;bir &#351;ey uydurma.&rdquo; 
            (Every answer must come DIRECTLY from the given text chunk. Do NOT add information that is NOT in the text &mdash; 
            make nothing up.) This ensures the training data contains only verified information from the actual ERP documentation.
        </div>

        <!-- ============================================ -->
        <h2 id="sec-5">5. PROMPT ENGINEERING (V1)</h2>
        <!-- ============================================ -->

        <p>The v1 generation pipeline uses a two-part prompt: a comprehensive system prompt (the &ldquo;Grand Prompt&rdquo;) 
        that defines the task, formats, rules, and examples; and a per-chunk user message that provides metadata 
        and the documentation text. The v2 prompt is significantly redesigned &mdash; see <a href="#sec-13">Section 13</a>.</p>

        <h3>System prompt structure</h3>
        <table>
            <tr><th>Section</th><th>Purpose</th></tr>
            <tr><td>S&#304;STEM HAKKINDA</td><td>Context about the ERP: 8 modules, cable factory, user types</td></tr>
            <tr><td>TEK G&Ouml;REV&#304;N</td><td>Single task definition: convert text to user-focused QA</td></tr>
            <tr><td>HEDEF MODEL HAKKINDA</td><td>26M parameter constraints: concise answers, no long paragraphs</td></tr>
            <tr><td>&#304;K&#304; T&#304;P VER&#304;</td><td>Output format: single-turn and multi-turn JSON schemas</td></tr>
            <tr><td>ZORLUK SEV&#304;YELER&#304;</td><td>Difficulty definitions: kolay (1-2 sent), orta (2-4), zor (4-6)</td></tr>
            <tr><td>ODAK NOKTASI</td><td>User-centric focus: &ldquo;How do I use the system?&rdquo; not code details</td></tr>
            <tr><td>KR&#304;T&#304;K KURALLAR</td><td>12 rules including anti-hallucination, translation, coverage</td></tr>
            <tr><td>&Ouml;RNEKLER</td><td>Good/bad examples showing desired vs undesired output style</td></tr>
        </table>

        <h3>Answer length calibration for v1 (26M parameters)</h3>
        <table>
            <tr><th>Difficulty</th><th>Target Length</th><th>Question Type</th><th>Example</th></tr>
            <tr><td class="good">Kolay (Easy)</td><td>1&ndash;2 sentences</td><td>Single fact: &ldquo;X nedir?&rdquo;</td><td>&ldquo;QR kod ne i&#351;e yarar?&rdquo;</td></tr>
            <tr><td>Orta (Medium)</td><td>2&ndash;4 sentences</td><td>Process/steps: &ldquo;X nas&#305;l yap&#305;l&#305;r?&rdquo;</td><td>&ldquo;Sisteme yeni bak&#305;r giri&#351;i nas&#305;l yap&#305;l&#305;r?&rdquo;</td></tr>
            <tr><td class="bad">Zor (Hard)</td><td>4&ndash;6 sentences</td><td>Multi-fact/scenario</td><td>&ldquo;Sipari&#351; tarihi de&#287;i&#351;irse ve k&#305;smi teslimat yap&#305;lm&#305;&#351;sa ne yapmal&#305;y&#305;m?&rdquo;</td></tr>
        </table>

        <div class="insight">
            <strong>Why calibrate length? (V1)</strong> A 26M parameter model cannot reliably generate long, complex answers. 
            By constraining answer lengths during data generation, the model learns patterns it can actually reproduce. 
            The v2 pipeline relaxes this for the 67.6M model: answers can be up to 200 words with flexible formatting
            (numbered steps, paragraphs, or lists), learned from examples rather than rigid templates.
        </div>

        <h3>Bad vs good question examples (from the prompt)</h3>
        <table>
            <tr><th>Type</th><th>Question</th><th>Problem / Reason</th></tr>
            <tr><td class="bad">BAD</td><td>&ldquo;raw_materials tablosunun s&uuml;tunlar&#305; nelerdir?&rdquo;</td><td>Too technical &mdash; DB schema, not user question</td></tr>
            <tr><td class="bad">BAD</td><td>&ldquo;POST /api/materials endpoint&rsquo;i ne d&ouml;nd&uuml;r&uuml;r?&rdquo;</td><td>API detail &mdash; users don&rsquo;t know endpoints</td></tr>
            <tr><td class="good">GOOD</td><td>&ldquo;QR kod ne i&#351;e yarar?&rdquo;</td><td>User-centric, natural language</td></tr>
            <tr><td class="good">GOOD</td><td>&ldquo;Sisteme yeni bak&#305;r giri&#351;i nas&#305;l yap&#305;l&#305;r?&rdquo;</td><td>Process-focused, practical</td></tr>
            <tr><td class="good">GOOD</td><td>&ldquo;Operat&ouml;r kalay teslim ald&#305;&#287;&#305;nda ne yapmal&#305;?&rdquo;</td><td>Scenario-based, role-aware</td></tr>
        </table>

        <!-- ============================================ -->
        <h2 id="sec-6">6. V1 QA PAIR STATISTICS</h2>
        <!-- ============================================ -->

        <p>The v1 generation script processed 320 out of 529 Turkish chunks before API credits were exhausted 
        (~$20 on Claude Opus 4.5). The resulting dataset was sufficient for the 26M parameter model.
        For v2 statistics (~8&ndash;10K pairs from 707 groups), see <a href="#sec-13">Section 13</a>.</p>

        <div class="stat-grid">
            <div class="stat-box">
                <div class="value good">3,790</div>
                <div class="label">TOTAL QA ITEMS</div>
            </div>
            <div class="stat-box">
                <div class="value">3,170</div>
                <div class="label">SINGLE-TURN (84%)</div>
            </div>
            <div class="stat-box">
                <div class="value">620</div>
                <div class="label">MULTI-TURN (16%)</div>
            </div>
            <div class="stat-box">
                <div class="value">11.7</div>
                <div class="label">AVG ITEMS PER CHUNK</div>
            </div>
        </div>

        <h3>Difficulty distribution</h3>
        <div class="stat-grid-3">
            <div class="stat-box">
                <div class="value">1,451</div>
                <div class="label">KOLAY / EASY (38%)</div>
            </div>
            <div class="stat-box">
                <div class="value">1,593</div>
                <div class="label">ORTA / MEDIUM (42%)</div>
            </div>
            <div class="stat-box">
                <div class="value">746</div>
                <div class="label">ZOR / HARD (20%)</div>
            </div>
        </div>

        <h3>Generation cost</h3>
        <table>
            <tr><th>Metric</th><th>Value</th></tr>
            <tr><td>Model used</td><td>Claude Opus 4.5 (<code>claude-opus-4-5-20251101</code>)</td></tr>
            <tr><td>Chunks processed</td><td>320 / 529 Turkish chunks (61%)</td></tr>
            <tr><td>API cost</td><td>~$20</td></tr>
            <tr><td>Generation rate</td><td>~24 items/minute</td></tr>
            <tr><td>Items per chunk</td><td>~11.7 average</td></tr>
            <tr><td>Output file</td><td><code>sft_data/erp_qa_pairs.jsonl</code></td></tr>
            <tr><td>ChatML file</td><td><code>sft_data/erp_sft_chatml.jsonl</code> (2.64 MB)</td></tr>
        </table>

        <!-- ============================================ -->
        <h2 id="sec-7">7. CHAT TEMPLATE &amp; TOKENIZATION</h2>
        <!-- ============================================ -->

        <p>The SFT data uses a Llama 3&ndash;style chat template, leveraging the special tokens already built into 
        the tokenizer during Phase 1. This was a deliberate design decision: the tokenizer was built with 
        instruction-tuning tokens <em>before the model existed</em>, anticipating this exact use case.</p>

        <h3>Special tokens used</h3>
        <table>
            <tr><th>Token</th><th>ID</th><th>Role in Chat Template</th></tr>
            <tr><td><code>&lt;|begin_of_text|&gt;</code></td><td>0</td><td>Start of conversation</td></tr>
            <tr><td><code>&lt;|start_header_id|&gt;</code></td><td>4</td><td>Opens role header (system/user/assistant)</td></tr>
            <tr><td><code>&lt;|end_header_id|&gt;</code></td><td>5</td><td>Closes role header</td></tr>
            <tr><td><code>&lt;|eot_id|&gt;</code></td><td>6</td><td>End of turn marker</td></tr>
            <tr><td><code>&lt;|pad|&gt;</code></td><td>2</td><td>Padding for batched training</td></tr>
        </table>

        <h3>Template structure</h3>
        <pre><span class="special">&lt;|begin_of_text|&gt;</span><span class="special">&lt;|start_header_id|&gt;</span>system<span class="special">&lt;|end_header_id|&gt;</span>

Sen Solen Kablo ERP sisteminin yapay zeka asistanısın...
<span class="special">&lt;|eot_id|&gt;</span><span class="special">&lt;|start_header_id|&gt;</span>user<span class="special">&lt;|end_header_id|&gt;</span>

QR kod ne işe yarar?
<span class="special">&lt;|eot_id|&gt;</span><span class="special">&lt;|start_header_id|&gt;</span>assistant<span class="special">&lt;|end_header_id|&gt;</span>

QR kod her hammaddeye atanan benzersiz bir takip kodudur...
<span class="special">&lt;|eot_id|&gt;</span></pre>

        <h3>Multi-turn extension</h3>
        <p>For multi-turn conversations (2&ndash;3 turns), the template simply repeats the user/assistant blocks. 
        Each turn ends with <code>&lt;|eot_id|&gt;</code>, and the model learns to generate until it produces this token.</p>

        <!-- ============================================ -->
        <h2 id="sec-8">8. LOSS MASKING STRATEGY</h2>
        <!-- ============================================ -->

        <p>A critical detail of SFT: loss is computed <strong>only on assistant tokens</strong>. The model must learn 
        to predict assistant responses, but it should not be penalized for failing to predict the system prompt 
        or user questions (which are given as input, not generated).</p>

        <h3>Token-level loss mask</h3>
        <pre><span class="comment">Token sequence:</span>
<span class="special">[BOS]</span> <span class="keyword">[HEADER:system]</span> system content... <span class="special">[EOT]</span>
      <span class="keyword">[HEADER:user]</span>   user question... <span class="special">[EOT]</span>
      <span class="keyword">[HEADER:asst]</span>   <span class="string">assistant answer...</span> <span class="special">[EOT]</span>

<span class="comment">Loss mask:</span>
  -1    -1  -1  -1  -1  ...  -1     <span class="comment">&larr; system (ignored)</span>
  -1    -1  -1  -1  -1  ...  -1     <span class="comment">&larr; user (ignored)</span>
  -1    -1  <span class="string">YES YES YES</span> ... <span class="string">YES</span>    <span class="comment">&larr; assistant content + EOT (trained)</span></pre>

        <p>The <code>ignore_index=-1</code> parameter in PyTorch&rsquo;s <code>cross_entropy</code> function handles this natively.
        Positions marked <code>-1</code> contribute zero to the loss. Only the assistant&rsquo;s content tokens and the 
        trailing <code>&lt;|eot_id|&gt;</code> are trained on.</p>

        <div class="finding">
            <strong>Why mask the header too?</strong> Even the <code>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n</code> 
            prefix is masked. The model doesn&rsquo;t need to learn to predict the role header &mdash; it will always be 
            provided as part of the prompt template. Training only on content maximizes the signal-to-noise ratio 
            for any model&rsquo;s parameter budget. This strategy is used for both v1 (26M) and v2 (67.6M).
        </div>

        <!-- ============================================ -->
        <h2 id="sec-9">9. V1 SFT TRAINING CONFIGURATION</h2>
        <!-- ============================================ -->

        <table>
            <tr><th>Parameter</th><th>Value</th><th>Rationale</th></tr>
            <tr><td><strong>Base checkpoint</strong></td><td>step_228000.pt</td><td>Best available pretrained model</td></tr>
            <tr><td><strong>Epochs</strong></td><td>3</td><td>Small dataset benefits from multiple passes</td></tr>
            <tr><td><strong>Batch size</strong></td><td>8 &times; 2 accum = 16</td><td>Effective batch of 16 conversations</td></tr>
            <tr><td><strong>Learning rate</strong></td><td>2e-5 &rarr; 2e-6</td><td>~15&times; lower than pretraining (3e-4)</td></tr>
            <tr><td><strong>LR schedule</strong></td><td>Cosine with warmup</td><td>63 warmup steps (10% of total)</td></tr>
            <tr><td><strong>Dropout</strong></td><td>0.05</td><td>Regularization for small dataset (was 0.0 in pretraining)</td></tr>
            <tr><td><strong>Weight decay</strong></td><td>0.01</td><td>Lower than pretraining (0.1)</td></tr>
            <tr><td><strong>Gradient clip</strong></td><td>1.0</td><td>Stability</td></tr>
            <tr><td><strong>Optimizer</strong></td><td>AdamW (fresh)</td><td>New optimizer state, not resumed from pretraining</td></tr>
            <tr><td><strong>Validation split</strong></td><td>10% (379 conversations)</td><td>Held-out evaluation after each epoch</td></tr>
        </table>

        <div class="stat-grid-3">
            <div class="stat-box">
                <div class="value">3,411</div>
                <div class="label">TRAIN CONVERSATIONS</div>
            </div>
            <div class="stat-box">
                <div class="value">379</div>
                <div class="label">VALIDATION CONVERSATIONS</div>
            </div>
            <div class="stat-box">
                <div class="value">141K</div>
                <div class="label">TRAINABLE TOKENS (ASSISTANT ONLY)</div>
            </div>
        </div>

        <div class="insight">
            <strong>Why a fresh optimizer?</strong> The pretrained model&rsquo;s AdamW momentum was tuned for next-token 
            prediction on raw Turkish text. SFT is a fundamentally different task (instruction following) with a much 
            smaller learning rate. Reusing the old optimizer state would inject stale momentum from pretraining, 
            potentially causing instability. A fresh optimizer allows clean adaptation.
        </div>

        <!-- ============================================ -->
        <h2 id="sec-10">10. V1 RESULTS</h2>
        <!-- ============================================ -->

        <div class="stat-grid">
            <div class="stat-box">
                <div class="value">5.12</div>
                <div class="label">VAL LOSS BEFORE SFT</div>
            </div>
            <div class="stat-box">
                <div class="value good">3.10</div>
                <div class="label">VAL LOSS AFTER SFT</div>
            </div>
            <div class="stat-box">
                <div class="value good">2.01</div>
                <div class="label">ABSOLUTE IMPROVEMENT</div>
            </div>
            <div class="stat-box">
                <div class="value good">39.5%</div>
                <div class="label">RELATIVE IMPROVEMENT</div>
            </div>
        </div>

        <h3>Per-epoch breakdown</h3>
        <table>
            <tr><th>Epoch</th><th>Train Loss</th><th>Val Loss</th><th>Best?</th><th>Time</th></tr>
            <tr><td>Before SFT</td><td>&mdash;</td><td>5.12</td><td></td><td>0s</td></tr>
            <tr><td>Epoch 1</td><td>8.53</td><td>3.34</td><td>&#9733;</td><td>28s</td></tr>
            <tr><td>Epoch 2</td><td>6.55</td><td>3.14</td><td>&#9733;</td><td>60s</td></tr>
            <tr class="highlight"><td><strong>Epoch 3</strong></td><td><strong>6.07</strong></td><td class="good"><strong>3.10</strong></td><td class="good"><strong>&#9733; Best</strong></td><td><strong>100s</strong></td></tr>
        </table>

        <p>Train loss is higher than val loss because the training loss is computed per-batch during gradient updates 
        (with dropout active), while validation loss is computed over the full validation set with dropout disabled.</p>

        <div class="discovery">
            <strong>Key result:</strong> In 100 seconds of SFT training on an H100 GPU, the model went from 
            producing incoherent repetitions (&ldquo;Digital program Digital program&hellip;&rdquo;) to generating 
            domain-relevant Turkish answers about the ERP system. The validation loss was still decreasing at epoch 3, 
            suggesting more epochs could yield further improvement.
        </div>

        <!-- ============================================ -->
        <h2 id="sec-11">11. V1 MODEL OUTPUTS</h2>
        <!-- ============================================ -->

        <h3>Before SFT (pretrained only)</h3>
        <div class="sample-box" style="border-color: #dc2626;">
            <div class="sample-label" style="color: #dc2626;">BEFORE SFT &mdash; Question: &ldquo;QR kod ne i&#351;e yarar?&rdquo;</div>
            <div class="sample-a" style="color: #999;">&ldquo;Digital program Digital program Digital program Digital program Digital program&hellip;&rdquo;</div>
        </div>
        <p>The pretrained model has no concept of the chat template, question-answering, or ERP knowledge. 
        Given the structured prompt, it falls into degenerate repetition.</p>

        <h3>After SFT (epoch 3)</h3>
        <div class="sample-box" style="border-color: #16a34a;">
            <div class="sample-label" style="color: #16a34a;">AFTER SFT &mdash; Question: &ldquo;QR kod ne i&#351;e yarar?&rdquo;</div>
            <div class="sample-q">S: QR kod ne i&#351;e yarar?</div>
            <div class="sample-a">&ldquo;QR kod, bir malzemenin hangi malzeme t&uuml;r&uuml;ne ait oldu&#287;unu takip eder.&rdquo;</div>
        </div>
        <div class="sample-box" style="border-color: #16a34a;">
            <div class="sample-label" style="color: #16a34a;">AFTER SFT &mdash; Question: &ldquo;Sisteme yeni bak&#305;r giri&#351;i nas&#305;l yap&#305;l&#305;r?&rdquo;</div>
            <div class="sample-q">S: Sisteme yeni bak&#305;r giri&#351;i nas&#305;l yap&#305;l&#305;r?</div>
            <div class="sample-a">&ldquo;Sistem bak&#305;r i&ccedil;in yeni bak&#305;r giri&#351;ini otomatik olarak hesaplar. Bu sayede bak&#305;r&#305;n t&uuml;m &uuml;retim s&uuml;reci takip edilir.&rdquo;</div>
        </div>
        <div class="sample-box" style="border-color: #16a34a;">
            <div class="sample-label" style="color: #16a34a;">AFTER SFT &mdash; Question: &ldquo;Hammadde mod&uuml;l&uuml;nde neler yapabilirim?&rdquo;</div>
            <div class="sample-q">S: Hammadde mod&uuml;l&uuml;nde neler yapabilirim?</div>
            <div class="sample-a">&ldquo;Hammadde mod&uuml;l&uuml;nde farkl&#305; hammaddeler i&ccedil;in malzemeler listesi olu&#351;turulur.&rdquo;</div>
        </div>
        <div class="sample-box" style="border-color: #16a34a;">
            <div class="sample-label" style="color: #16a34a;">AFTER SFT &mdash; Question: &ldquo;Sat&#305;&#351; sipari&#351;i nas&#305;l olu&#351;turulur?&rdquo;</div>
            <div class="sample-q">S: Sat&#305;&#351; sipari&#351;i nas&#305;l olu&#351;turulur?</div>
            <div class="sample-a">&ldquo;Sat&#305;&#351; sipari&#351;i otomatik olarak a&ccedil;&#305;l&#305;r men&uuml;de g&ouml;sterilir. Bu men&uuml;de sipari&#351; detaylar&#305;, tarih ve saat alanlar&#305;, teslim edilen miktar, teslimat tarihi alanlar&#305;na a&#287;&#305;rl&#305;k ayarlar&#305;yla birlikte g&ouml;sterilir.&rdquo;</div>
        </div>

        <div class="finding">
            <strong>Observations:</strong> The model (1) speaks coherent Turkish with correct grammar and suffixes, 
            (2) stays on-topic about ERP operations, (3) uses domain vocabulary correctly (malzeme, sipari&#351;, teslimat, 
            mod&uuml;l), and (4) follows the question-answer pattern learned during SFT. Answers are concise, matching 
            the difficulty-calibrated length targets. Some answers remain somewhat generic &mdash; expected after only 
            3 epochs of SFT on a 26M model.
        </div>

        <!-- ============================================ -->
        <h2 id="sec-12">12. ANALYSIS &amp; LESSONS LEARNED</h2>
        <!-- ============================================ -->

        <h3>What v1 SFT proved</h3>
        <ul>
            <li>Turkish grammar and morphology are solid &mdash; learned during pretraining, preserved through SFT</li>
            <li>Domain vocabulary (hammadde, sipari&#351;, tedarik&ccedil;i, malzeme) is correctly used</li>
            <li>Chat template pattern (system &rarr; user &rarr; assistant) is reliably followed</li>
            <li>The model stops generating at the right point (produces <code>&lt;|eot_id|&gt;</code>)</li>
            <li>SFT on a tiny model is fast (&lt;2 minutes) and cheap (&lt;$0.10), enabling rapid iteration</li>
        </ul>

        <h3>V1 limitations that drove the v2 redesign</h3>
        <table>
            <tr><th>V1 Limitation</th><th>Root Cause</th><th>V2 Solution</th></tr>
            <tr>
                <td class="bad">Repetitive outputs</td>
                <td>26M model capacity + no dropout + 512-token context</td>
                <td class="good">67.6M model + dropout 0.02 + 2048 context</td>
            </tr>
            <tr>
                <td class="bad">Generic, ungrounded answers</td>
                <td>No RAG context in training &mdash; model guesses from memorized patterns</td>
                <td class="good">Every training example includes the source chunk as context</td>
            </tr>
            <tr>
                <td class="bad">Only 60% chunk coverage</td>
                <td>API credits exhausted at 320/529 chunks</td>
                <td class="good">All 532 chunks processed via 707 groups across 11 rules</td>
            </tr>
            <tr>
                <td class="bad">Single-chunk questions only</td>
                <td>No grouping strategy &mdash; each chunk processed individually</td>
                <td class="good">11 grouping rules (submodule, cross-module, data flow, etc.)</td>
            </tr>
            <tr>
                <td class="bad">No audience diversity</td>
                <td>V1 prompt targeted &ldquo;office worker&rdquo; questions only</td>
                <td class="good">V2 generates both user-level and technical questions</td>
            </tr>
            <tr>
                <td>Sonnet 4 / Opus 4.5 API</td>
                <td>Good but not optimal for Turkish Q&amp;A diversity</td>
                <td class="good">Sonnet 4.6 selected via 4-way comparative pilot</td>
            </tr>
        </table>

        <h3>Key insight: SFT is the critical phase at this scale</h3>
        <div class="insight">
            <strong>For models under ~1B parameters, SFT training data quality is the single most important factor.</strong>
            RL techniques (DPO, RLHF, RLVR) provide diminishing returns at this scale because the model lacks the
            capacity to benefit from fine-grained preference signals. Instead of chasing RL, the strategy is:
            (1) maximize SFT data quality via better prompts, diverse grouping, and superior API models;
            (2) improve the retrieval pipeline so the model sees better context at inference time;
            (3) iterate on the RAG prompt engineering for the deployed system.
            RL remains an optional future phase if SFT alone proves insufficient.
        </div>

        <div class="warning">
            <strong>Experiment: intentional wrong answers in SFT.</strong> An earlier experiment injected incorrect
            answers that self-corrected mid-response (e.g., &ldquo;wait, this is wrong&hellip;&rdquo;). Result:
            overall model quality degraded significantly. <strong>Never inject bad answers into SFT data.</strong>
            If preference learning is needed, use DPO with separate (chosen, rejected) pairs instead.
        </div>

        <!-- ============================================ -->
        <h2 id="sec-13">13. V2 SFT PIPELINE REDESIGN</h2>
        <!-- ============================================ -->

        <p>The v1 SFT (3,790 pairs from Claude Opus 4.5) proved the concept but exposed limitations:
        only 320 of 529 chunks were processed before API credits ran out, the grouping strategy was
        basic (individual chunks only), and the 512-token context couldn&rsquo;t fit real RAG prompts.
        The v2 pipeline was redesigned from scratch for the 67.6M RAG-optimized model.</p>

        <div class="stat-grid-5">
            <div class="stat-box">
                <div class="value good">707</div>
                <div class="label">CHUNK GROUPS</div>
            </div>
            <div class="stat-box">
                <div class="value good">11</div>
                <div class="label">GROUPING RULES</div>
            </div>
            <div class="stat-box">
                <div class="value">532</div>
                <div class="label">SOURCE CHUNKS</div>
            </div>
            <div class="stat-box">
                <div class="value good">408K</div>
                <div class="label">INPUT TOKENS</div>
            </div>
            <div class="stat-box">
                <div class="value good">7,595</div>
                <div class="label">QA PAIRS GENERATED</div>
            </div>
        </div>

        <h3>Why redesign?</h3>
        <ul>
            <li><strong>Coverage gap:</strong> v1 only processed 60% of chunks. v2 processes all 532 chunks in multiple configurations.</li>
            <li><strong>Single-chunk limitation:</strong> v1 generated Q&amp;A from individual chunks only. v2 uses 11 grouping rules that combine chunks by submodule, module, cross-module, data flow, and more &mdash; producing questions that require synthesizing information across chunks.</li>
            <li><strong>Model upgrade:</strong> v2 targets a 67.6M model with 2048-token context, enabling much richer RAG prompts with longer context chunks.</li>
            <li><strong>API model quality:</strong> v1 used Claude Opus 4.5. A 4-way comparison showed Claude Sonnet 4.6 produces superior question diversity and inference depth.</li>
        </ul>

        <h3>Master grouping strategy: 11 rules</h3>
        <p>Each rule produces a different perspective on the same documentation, forcing diverse question types:</p>
        <table>
            <tr><th>#</th><th>Rule</th><th>Groups</th><th>Tokens</th><th>Description</th></tr>
            <tr><td>1</td><td><strong>Individual</strong></td><td>532</td><td>107K</td><td>Each chunk alone &mdash; factual, definition, basic procedure questions</td></tr>
            <tr><td>2</td><td><strong>Submodule</strong></td><td>86</td><td>107K</td><td>Chunks grouped by submodule &mdash; cross-chunk synthesis within a feature</td></tr>
            <tr><td>3</td><td><strong>Module</strong></td><td>8</td><td>107K</td><td>All chunks per module &mdash; high-level architectural questions</td></tr>
            <tr><td>4</td><td><strong>Vertical stack</strong></td><td>10</td><td>13K</td><td>Same feature at different depths (overview &rarr; detail &rarr; API)</td></tr>
            <tr><td>5</td><td><strong>Horizontal siblings</strong></td><td>16</td><td>18K</td><td>Parallel features at same depth &mdash; comparison questions</td></tr>
            <tr><td>6</td><td><strong>Cross-module</strong></td><td>6</td><td>7K</td><td>Related features across different modules</td></tr>
            <tr><td>7</td><td><strong>Overview + detail</strong></td><td>18</td><td>18K</td><td>Module overview paired with specific submodule details</td></tr>
            <tr><td>8</td><td><strong>Data flow chain</strong></td><td>4</td><td>7K</td><td>Sequential process chains (e.g., order &rarr; production &rarr; delivery)</td></tr>
            <tr><td>9</td><td><strong>Foundation + consumer</strong></td><td>10</td><td>6K</td><td>Base definitions paired with features that use them</td></tr>
            <tr><td>10</td><td><strong>Shared DB tables</strong></td><td>13</td><td>16K</td><td>Features sharing database tables &mdash; data integration questions</td></tr>
            <tr><td>11</td><td><strong>Module map</strong></td><td>4</td><td>3K</td><td>Full module structure maps for navigation questions</td></tr>
            <tr class="highlight"><td></td><td><strong>Total</strong></td><td><strong>707</strong></td><td class="good"><strong>408K</strong></td><td></td></tr>
        </table>

        <div class="discovery">
            <strong>Distribution note.</strong> Rule 1 (individual chunks) produces 75% of all groups (532/707)
            and ~80% of expected QA pairs. This is intentional: the majority of real user queries will be
            answerable from a single retrieved chunk. The remaining 10 rules (175 groups, ~20% of pairs)
            train the model on harder multi-chunk reasoning that occurs when the retriever returns related
            but separate passages.
        </div>

        <h3>API model comparison: 4-way pilot</h3>
        <p>Before committing to a single API model for all 707 groups, a controlled pilot was run on the
        same 10 chunk groups with 4 different models:</p>
        <table>
            <tr><th>Model</th><th>Pairs</th><th>Question Diversity</th><th>Answer Depth</th><th>Inference Quality</th><th>Repetition</th></tr>
            <tr><td><strong>Claude Sonnet 4</strong></td><td>99</td><td class="bad">Low &mdash; mostly factual</td><td>Adequate</td><td class="bad">Shallow</td><td>Some</td></tr>
            <tr><td><strong>GPT-5.2</strong></td><td>101</td><td>Moderate</td><td>Good</td><td>Good</td><td>Minimal</td></tr>
            <tr><td><strong>Claude Opus 4.6</strong></td><td>100</td><td>Good</td><td class="good">Excellent</td><td class="good">Very good</td><td>Minimal</td></tr>
            <tr class="highlight"><td class="good"><strong>Claude Sonnet 4.6</strong></td><td>100</td><td class="good">Excellent</td><td class="good">Excellent</td><td class="good">Best &mdash; deep inference</td><td class="good">Minimal</td></tr>
        </table>

        <div class="finding">
            <strong>Winner: Claude Sonnet 4.6.</strong> Sonnet 4.6 produced the most diverse question types
            (factual, procedural, comparative, inferential, list) with the deepest inference-based questions.
            It consistently generated questions that require combining information from multiple parts of the
            context, rather than simple lookups. Sonnet 4 was notably weaker &mdash; its questions were almost
            entirely factual with shallow answers. Opus 4.6 was close but slightly less diverse.
        </div>

        <h3>V2 prompt engineering</h3>
        <p>The data generation uses a two-level prompt architecture:</p>

        <p><strong>1. SFT system prompt</strong> (embedded in every training example, 38 tokens):</p>
        <pre>ERP sistemi asistanısın. Verilen bağlam bilgilerini kullanarak
soruyu yanıtla. Bağlamda cevap yoksa "Bu konuda bilgim yok" de.</pre>

        <div class="insight">
            <strong>System prompt design decisions:</strong>
            <ul style="margin-top: 8px;">
                <li>Proper Turkish characters (ı, ş, ü, ö, ç, ğ) &mdash; v1&rsquo;s <code>SYSTEM_TR</code> used ASCII approximations, fixed here</li>
                <li>No audience restriction &mdash; model answers both office workers and technical users</li>
                <li>No format instructions &mdash; the model learns formatting from examples, not from the system prompt</li>
                <li>Built-in grounding: &ldquo;if not in context, say I don&rsquo;t know&rdquo;</li>
                <li>38 tokens is ultra-short &mdash; critical for a 2048-token context budget</li>
            </ul>
        </div>

        <p><strong>2. API system prompt</strong> (sent to Claude Sonnet 4.6, not seen by the model):</p>
        <p>A detailed Turkish-language instruction set covering:</p>
        <ul>
            <li><strong>6 core rules:</strong> context-only answers, natural questions, complete/correct, use technical terms as-is, independent pairs, no repetition</li>
            <li><strong>5 question types:</strong> olgusal (factual), pros&uuml;d&uuml;rel (procedural), karşılaştırmalı (comparative), çıkarımsal (inferential), liste (list)</li>
            <li><strong>4 answer formats:</strong> numbered steps for procedures, short paragraphs for definitions, bullet lists for enumerations, concise explanations for comparisons</li>
            <li><strong>3 difficulty levels:</strong> kolay ~40% (single-fact lookup), orta ~40% (multi-fact synthesis), zor ~20% (inference/comparison)</li>
            <li><strong>Dynamic pair count:</strong> 5&ndash;28 pairs per group based on input token count</li>
        </ul>

        <h3>V2 generation: final results</h3>

        <div class="stat-grid-5">
            <div class="stat-box">
                <div class="value good">7,595</div>
                <div class="label">TOTAL QA PAIRS</div>
            </div>
            <div class="stat-box">
                <div class="value good">707/707</div>
                <div class="label">GROUPS COMPLETE</div>
            </div>
            <div class="stat-box">
                <div class="value">10.7</div>
                <div class="label">AVG PAIRS/GROUP</div>
            </div>
            <div class="stat-box">
                <div class="value">0</div>
                <div class="label">INVALID PAIRS</div>
            </div>
            <div class="stat-box">
                <div class="value good">44.9 MB</div>
                <div class="label">RAW DATA SIZE</div>
            </div>
        </div>

        <table>
            <tr><th>Metric</th><th>Value</th></tr>
            <tr><td>Groups processed</td><td class="good">707 / 707 (100%)</td></tr>
            <tr><td>QA pairs generated</td><td class="good">7,595</td></tr>
            <tr><td>Invalid pairs</td><td class="good">0</td></tr>
            <tr><td>Parse errors (retried)</td><td>2 (both resolved on retry)</td></tr>
            <tr><td>Avg / Min / Max pairs per group</td><td>10.7 / 5 / 29</td></tr>
        </table>

        <h3>Question type distribution</h3>
        <table>
            <tr><th>Type</th><th>Count</th><th>Percentage</th></tr>
            <tr><td>Olgusal (factual)</td><td>3,850</td><td>50.7%</td></tr>
            <tr><td>&Ccedil;&#305;kar&#305;msal (inferential)</td><td>1,164</td><td>15.3%</td></tr>
            <tr><td>Liste (list)</td><td>999</td><td>13.2%</td></tr>
            <tr><td>Pros&uuml;d&uuml;rel (procedural)</td><td>896</td><td>11.8%</td></tr>
            <tr><td>Kar&#351;&#305;la&#351;t&#305;rmal&#305; (comparative)</td><td>686</td><td>9.0%</td></tr>
        </table>

        <h3>Difficulty distribution</h3>
        <div class="stat-grid-3">
            <div class="stat-box">
                <div class="value">3,791</div>
                <div class="label">KOLAY / EASY (49.9%)</div>
            </div>
            <div class="stat-box">
                <div class="value">2,629</div>
                <div class="label">ORTA / MEDIUM (34.6%)</div>
            </div>
            <div class="stat-box">
                <div class="value">1,175</div>
                <div class="label">ZOR / HARD (15.5%)</div>
            </div>
        </div>

        <h3>Pairs by grouping rule</h3>
        <table>
            <tr><th>#</th><th>Rule</th><th>Groups</th><th>Pairs</th><th>Avg/Group</th></tr>
            <tr><td>1</td><td>Individual</td><td>532</td><td>4,349</td><td>8.2</td></tr>
            <tr><td>2</td><td>Submodule</td><td>86</td><td>1,445</td><td>16.8</td></tr>
            <tr><td>3</td><td>Module</td><td>8</td><td>228</td><td>28.5</td></tr>
            <tr><td>4</td><td>Vertical stack</td><td>10</td><td>203</td><td>20.3</td></tr>
            <tr><td>5</td><td>Horizontal siblings</td><td>16</td><td>309</td><td>19.3</td></tr>
            <tr><td>6</td><td>Cross-module</td><td>6</td><td>120</td><td>20.0</td></tr>
            <tr><td>7</td><td>Overview + detail</td><td>18</td><td>351</td><td>19.5</td></tr>
            <tr><td>8</td><td>Data flow chain</td><td>4</td><td>99</td><td>24.8</td></tr>
            <tr><td>9</td><td>Foundation + consumer</td><td>10</td><td>150</td><td>15.0</td></tr>
            <tr><td>10</td><td>Shared DB tables</td><td>13</td><td>271</td><td>20.8</td></tr>
            <tr><td>11</td><td>Module map</td><td>4</td><td>70</td><td>17.5</td></tr>
            <tr class="highlight"><td></td><td><strong>Total</strong></td><td><strong>707</strong></td><td class="good"><strong>7,595</strong></td><td><strong>10.7</strong></td></tr>
        </table>

        <div class="discovery">
            <strong>V1 &rarr; V2 final comparison.</strong> V1 produced 3,790 pairs from 320 individual chunks
            using Claude Opus 4.5. V2 produced <strong>7,595 pairs</strong> from 707 multi-configuration groups
            using Claude Sonnet 4.6 &mdash; a <strong>2.0&times;</strong> increase in data volume with substantially
            higher question diversity (11 grouping rules vs 1, 5 question types, 3 difficulty levels).
        </div>

        <table>
            <tr><th>Metric</th><th>V1</th><th>V2</th></tr>
            <tr><td>API model</td><td>Claude Opus 4.5</td><td class="good">Claude Sonnet 4.6</td></tr>
            <tr><td>Chunks processed</td><td>320 / 529 (60%)</td><td class="good">532 / 532 (100%)</td></tr>
            <tr><td>Grouping rules</td><td>1 (individual only)</td><td class="good">11 rules</td></tr>
            <tr><td>Total groups</td><td>320</td><td class="good">707</td></tr>
            <tr><td>QA pairs</td><td>3,790</td><td class="good">7,595</td></tr>
            <tr><td>Question types</td><td>Mixed, unstructured</td><td class="good">5 types, difficulty-graded</td></tr>
            <tr><td>RAG context in training</td><td class="bad">No</td><td class="good">Yes &mdash; every example</td></tr>
            <tr><td>Output file</td><td><code>sft_data/erp_qa_pairs.jsonl</code></td><td><code>erp_rag/data/sft_raw_pairs.json</code> (44.9 MB)</td></tr>
            <tr><td>ChatML file</td><td><code>sft_data/erp_sft_chatml.jsonl</code> (2.64 MB)</td><td><code>erp_rag/data/sft_train.jsonl</code> (45.8 MB)</td></tr>
        </table>

        <!-- ============================================ -->
        <h2 id="sec-14">14. REPRODUCIBILITY</h2>
        <!-- ============================================ -->

        <h3>Complete file inventory</h3>

        <p><strong>V1 SFT files:</strong></p>
        <table>
            <tr><th>File</th><th>Purpose</th><th>Size</th></tr>
            <tr><td><code>scripts/generate_erp_qa.py</code></td><td>V1 QA generation (Claude Opus 4.5)</td><td>~12 KB</td></tr>
            <tr><td><code>sft_data/erp_qa_pairs.jsonl</code></td><td>V1 raw QA pairs</td><td>~3 MB</td></tr>
            <tr><td><code>sft_data/erp_sft_chatml.jsonl</code></td><td>V1 ChatML training data</td><td>2.64 MB</td></tr>
            <tr><td><code>tiny_llm/train_sft.py</code></td><td>V1 SFT training loop</td><td>~14 KB</td></tr>
            <tr><td><code>tiny_llm/checkpoints/sft/sft_best.pt</code></td><td>V1 best SFT checkpoint (epoch 3)</td><td>94 MB</td></tr>
        </table>

        <p><strong>V2 SFT files:</strong></p>
        <table>
            <tr><th>File</th><th>Purpose</th><th>Size</th></tr>
            <tr><td><code>erp_rag/generate/sft_generate.py</code></td><td>V2 data generation pipeline (Claude Sonnet 4.6)</td><td>~18 KB</td></tr>
            <tr><td><code>erp_rag/data/sft_chunk_groups.json</code></td><td>Master grouping blueprint (707 groups, 11 rules)</td><td>~400 KB</td></tr>
            <tr><td><code>erp_rag/data/sft_raw_pairs.json</code></td><td>V2 raw QA pairs with context</td><td>44.9 MB</td></tr>
            <tr><td><code>erp_rag/data/sft_train.jsonl</code></td><td>V2 ChatML training data (7,595 examples)</td><td>45.8 MB</td></tr>
            <tr><td><code>tiny_llm/train_sft_rag.py</code></td><td>V2 RAG-grounded SFT training loop</td><td>~20 KB</td></tr>
        </table>

        <p><strong>Shared files:</strong></p>
        <table>
            <tr><th>File</th><th>Purpose</th><th>Size</th></tr>
            <tr><td><code>tiny_llm/sft_data.py</code></td><td>Tokenization and assistant-only loss masking</td><td>~4 KB</td></tr>
            <tr><td><code>tiny_llm/chat.py</code></td><td>Interactive chat with SFT model</td><td>~5 KB</td></tr>
            <tr><td><code>erp_rag/data/chunks/all_chunks.json</code></td><td>Pre-processed ERP documentation (1,074 chunks)</td><td>1.9 MB</td></tr>
        </table>

        <h3>To reproduce</h3>
        <pre><span class="comment"># V2 pipeline (recommended)</span>

<span class="comment"># 1. Generate QA pairs (requires Anthropic API key, ~$20-25)</span>
<span class="keyword">pip install</span> anthropic
<span class="keyword">export</span> ANTHROPIC_API_KEY=<span class="string">"sk-ant-..."</span>
python -m erp_rag.generate.sft_generate \
    --provider anthropic --model claude-sonnet-4-6   <span class="comment"># 707 groups → 7,595 pairs, auto-resume on interruption</span>

<span class="comment"># 2. Upload data to RunPod and run SFT training</span>
python -m tiny_llm.train_sft_rag \
    --model v2 \
    --data erp_rag/data/sft_raw_pairs.json           <span class="comment"># reads raw pairs, builds ChatML, trains with loss masking</span>

<span class="comment"># 3. Chat with the model</span>
python -m tiny_llm.chat</pre>

        <h3>Experiments tried &amp; decisions locked</h3>

        <div class="warning">
            <strong>DO NOT RE-TRY:</strong> These SFT experiments and configurations were already evaluated.
        </div>

        <table>
            <tr><th>Experiment</th><th>What Was Tried</th><th>Result</th><th>Decision</th></tr>
            <tr>
                <td><strong>SFT on local Mac (MPS)</strong></td>
                <td>Ran <code>train_sft.py</code> on M4 MacBook</td>
                <td class="bad">CPU hit 93&deg;C; killed immediately</td>
                <td class="good">RunPod H100 only &mdash; locked</td>
            </tr>
            <tr>
                <td><strong>torch.compile checkpoint loading</strong></td>
                <td>Loaded step_228000.pt directly into non-compiled model</td>
                <td class="bad">All keys mismatched (<code>_orig_mod.</code> prefix); loss ~20.0</td>
                <td class="good">Always strip <code>_orig_mod.</code> prefix &mdash; locked</td>
            </tr>
            <tr>
                <td><strong>V1: Claude Opus 4.5</strong></td>
                <td>320/529 chunks processed before API credits exhausted ($20)</td>
                <td>3,790 QA pairs (3,170 single + 620 multi-turn)</td>
                <td>Superseded by V2 pipeline</td>
            </tr>
            <tr>
                <td><strong>V2: Claude Sonnet 4.6</strong></td>
                <td>707/707 groups, 11 rules, 532 chunks, ~$20&ndash;25</td>
                <td class="good">7,595 QA pairs, 0 invalid, 5 types, 3 difficulty levels</td>
                <td class="good">V2 pipeline &mdash; locked. Production-ready SFT data.</td>
            </tr>
            <tr>
                <td><strong>SFT dropout 0.05</strong></td>
                <td>Increased from pretraining&rsquo;s 0.0</td>
                <td class="good">Val loss improved steadily across 3 epochs; no overfitting</td>
                <td class="good">dropout 0.05 for SFT &mdash; locked</td>
            </tr>
            <tr>
                <td><strong>LR 2e-5 for SFT</strong></td>
                <td>10&times; lower than pretraining peak (3e-4)</td>
                <td class="good">Stable training; val loss 5.12 &rarr; 3.10 across 3 epochs</td>
                <td class="good">LR 2e-5 for SFT &mdash; locked</td>
            </tr>
            <tr>
                <td><strong>3 SFT epochs</strong></td>
                <td>Trained for 3 full epochs over 3,790 conversations</td>
                <td>Best val loss at epoch 3 (3.10); still improving</td>
                <td>Could train more epochs, but model already follows instructions</td>
            </tr>
            <tr>
                <td><strong>Fresh optimizer for SFT</strong></td>
                <td>New AdamW (did not carry pretraining optimizer state)</td>
                <td class="good">Correct approach: SFT loss surface differs from pretraining</td>
                <td class="good">Fresh optimizer for SFT &mdash; locked</td>
            </tr>
            <tr>
                <td><strong>Loss masking (assistant-only)</strong></td>
                <td>IGNORE=-1 for system/user tokens; only assistant+EOT contribute to loss</td>
                <td class="good">Model learns to generate answers, not memorize questions</td>
                <td class="good">Assistant-only loss &mdash; locked</td>
            </tr>
        </table>

        <div class="insight">
            <strong>Cost breakdown for the entire project (Phases 1&ndash;4):</strong>
            Tokenizer training: free (CPU).
            Pretraining v1 (R1+R2+R2.5): ~$92.83 (39h &times; $2.38/hr).
            V1 SFT data generation: ~$20 (Claude Opus 4.5 API).
            V1 SFT training: &lt;$0.10 (100 seconds on H100).
            V2 SFT data generation: ~$22 (Claude Sonnet 4.6 API, 707 calls).
            <strong>Total so far: ~$135 (excluding v2 pretraining).</strong>
        </div>

        <div class="abstract" style="margin-top: 32px;">
            <strong>Conclusion.</strong> Two generations of SFT have been built for this project. V1 proved the concept:
            3,790 QA pairs from Claude Opus 4.5 transformed a 24.7M pretrained model into a functional ERP assistant
            in 100 seconds. V2 redesigns everything at scale: 11 grouping strategies produce 707 chunk groups from 532
            ERP documentation chunks; Claude Sonnet 4.6 (selected via 4-way comparative pilot) generates ~8,000&ndash;10,000
            diverse QA pairs spanning factual, procedural, comparative, inferential, and list-type questions at three
            difficulty levels. The target model (67.6M v2 with 2048-token context) was purpose-built as a RAG context
            converter. The entire pipeline &mdash; tokenizer, architecture, pretraining, data generation, SFT training &mdash;
            remains built from scratch with no pretrained weights, no HuggingFace trainers, and no off-the-shelf datasets.
            V2 SFT data generation is complete: 7,595 pairs, ready for training.
            <strong>Total project cost to date: ~$135.</strong>
        </div>

        <p style="text-align: center; margin-top: 40px; font-size: 11px; color: #888;">
            &copy; 2026 &bull; Independent Research
        </p>
    </main>

</body>
</html>
